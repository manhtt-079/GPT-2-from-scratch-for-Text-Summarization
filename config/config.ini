[dataset]
max_seq_length = 512
file_path = ./data/jp_text_sum.csv

[gpt2-conf]
model_checkpoint = rinna/japanese-gpt2-medium
vocab_size = 32000
n_positions = 1024
n_embeds = 1024
n_layers = 12
n_heads = 16
n_inners = 4096
activation_func = gelu
residual_dropout = 0.1
embed_dropout = 0.1
attn_dropout = 0.1
layer_norm_epsilon = 1e-5


[trainer]
accelerator = gpu
accumulate_grad_batches = 8
amp_backend = native
auto_lr_find = False
auto_scale_batch_size = False
auto_select_gpus = False
batch_size = 1
checkpoint = ./checkpoint/
default_root_dir = ./checkpoint
delta = 1.0e-5
devices = 1
enable_model_summary = True
enable_checkpointing = True
enable_progess_bar = True
eval_steps = 50
monitor = val_loss
no_decay = bias,ln.weight,ln_1.weight,ln_2.weight
log_every_n_steps = 128
log = ./log/
lr = 3e-4
losses = CrossEntropyLoss
max_epochs = 20
num_workers = 16
patience = 5
precision = 16
random_state = 42
save_on_train_epoch_end = True
save_top_k = 3
warmup_ratio = 0.01
weight_decay = 0.015